{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from os.path import basename\n",
    "import ast\n",
    "from itertools import product\n",
    "import re\n",
    "from string import Formatter\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "source_format = \"C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_output/bws_{}*\"\n",
    "target_format = \"C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_cache/{}\"\n",
    "version_param  = \"V1-9\"\n",
    "clear_data = True\n",
    "\n",
    "def m_columns(cols): return [c for c in cols if c[0:1]==\"M\" and c[0:2]!= \"M0\"]\n",
    "def ensure_dir(d): \n",
    "    if not os.path.exists(d): os.makedirs(d)\n",
    "def make_key(t):  \n",
    "    d = dict(t)\n",
    "    s= \"H\"+str(d[\"H\"])+\"D\"+str(int(d[\"D\"]))+\"B\"+str(int(d[\"BCs\"]))\n",
    "    return s.replace(\".\",\"\")\n",
    "\n",
    "def read_df_custom(f):\n",
    "    lines = []\n",
    "    #print(\"opening file \", f)\n",
    "    with open(f) as fl:\n",
    "        for i,l in enumerate(fl):\n",
    "            if l.startswith('#'):\n",
    "                continue\n",
    "            l = l.replace(\"\\n\", \"\").replace('\\n', \"\")\n",
    "            lines.append(l.split('\\t'))\n",
    "    df = pd.DataFrame([l for l in lines[1:-1]], columns = lines[0] ).astype(float)\n",
    "    return df\n",
    "\n",
    "def _parse_params(l):\n",
    "    \"\"\"\n",
    "    Assumes parameter line is a valid dictionary/json like format and parses it \n",
    "    otherwise just returns the full line\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if l.replace(\" \", \"\").startswith(\"#Parameters\"):\n",
    "        l = l.replace(\"#Parameters=\", \"\")\n",
    "        l = l.replace(\"# Parameters=\", \"\")\n",
    "        l=l.replace(\" \",\"\").replace(\"(\",\"\").replace(\")\",\"\")            \n",
    "        try:\n",
    "            d = ast.literal_eval(l)\n",
    "            #if we needed to map names we would do it here\n",
    "            return d\n",
    "        \n",
    "        except:\n",
    "            try:\n",
    "                #try look for terms\n",
    "                terms = l.replace(\"{\",\"\").replace(\"}\",\"\").split(\",\")\n",
    "                for t in terms:\n",
    "                    a,b = t.split(\":\")\n",
    "                    a = a.lstrip().rstrip().replace(\" \",\"\")\n",
    "                    #print(\"parsing terms\",a)\n",
    "                    #we use the non-spaced tokens\n",
    "                    cl_no_space = [s.replace(\" \",\"\") for s in [\"Hopping rate\", \"Realisations\", \"Chunk size\"]]\n",
    "                    if a in cl_no_space: \n",
    "                        d[a] = b\n",
    "            except:\n",
    "                return {\"unparsed\", p}\n",
    "            \n",
    "    return d\n",
    "        \n",
    "    \n",
    "def get_file_list(vkey, format_string):\n",
    "    search_path = format_string.format(vkey)\n",
    "    print(\"searching for files\", search_path)\n",
    "    for f in glob(search_path):\n",
    "        yield f\n",
    "        \n",
    "        \n",
    "def read_info(file, param_line=\"#Parameters\",sep='\\t'):\n",
    "    with open(file) as _f:\n",
    "        d = {\"version\": \"default\"}\n",
    "        #TODO: extract version and other info from file header\n",
    "        \n",
    "        tokens = file.split(\"-\") #should use a smarter regex maybe    \n",
    "        if len(tokens) > 1: d[\"version\"]= tokens[0].split(\"_\")[-1]+\"-\"+tokens[1]\n",
    "        for i,line in enumerate(_f):\n",
    "            if line.startswith(param_line):\n",
    "                d.update(_parse_params(line))                    \n",
    "            if i == 10:#some reasonable comment scanning\n",
    "                break\n",
    "        d[\"file\"] = file\n",
    "        return d  #{\"file\":f, \"L\" : None, \"D\": None, \"BCs\" : None, \"seed\" : None, \"N\":None, \"h\":None, \"T\"} #info\n",
    "\n",
    "\n",
    "def process(version_param, source_format,target_format):\n",
    "    for f in get_file_list(version_param, source_format): \n",
    "        info = read_info(f)\n",
    "        #print(info)\n",
    "        H = info[\"Hoppingrate\"]\n",
    "        df = read_df_custom(f)\n",
    "        for k,g in df.groupby([\"L\",\"D\",\"BCs\"]): \n",
    "            key = tuple(zip([\"L\",\"D\",\"BCs\", \"H\"], list(k)+[H]))\n",
    "            print(\"saving data for\", key, \"to the cache location\")\n",
    "            moment_cols = m_columns(g.columns)\n",
    "            mn = g.groupby(\"t\").mean()[moment_cols]\n",
    "            st = g.groupby(\"t\").std()[moment_cols] / np.sqrt(g.groupby(\"t\").count())[moment_cols]\n",
    "            g = mn.join(st, rsuffix=\"error\")\n",
    "            group_name=make_key(key)\n",
    "            directory = (target_format+\"/{}/\").format(version_param,group_name)\n",
    "            ensure_dir(directory)\n",
    "            g.to_csv(directory+\"L\"+str(int(key[0][1]))+\".csv\")\n",
    "\n",
    "    print(\"data saved to cache, see\", target_format.format(version_param))\n",
    "    print(\"combing data in all folders\")\n",
    "    for directory in glob(target_format.format(version_param)+\"/*\"):\n",
    "        print(\"merging\", directory)\n",
    "        data = {}\n",
    "        for f in glob(directory+\"/L*.*\"):\n",
    "            L = int(basename(f).split(\".\")[0][1:])\n",
    "            df= pd.read_csv(f).set_index(\"t\")\n",
    "            L = \"{:0>5}\".format(L)\n",
    "            sorted_cols = df.columns.values\n",
    "            sorted_cols.sort()\n",
    "            df = df[sorted_cols]\n",
    "            df.columns = [L+c for c in df.columns]\n",
    "            data[int(L)] = df\n",
    "            P = Path(directory)\n",
    "        ordered_l = list(data.keys())\n",
    "        ordered_l.sort()\n",
    "        pd.concat([data[i] for i in ordered_l],axis=1).to_csv(directory+\"/{}_ALL{}.csv\".format(version_param,P.parts[-1]))\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for files C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_output/bws_V1-8*\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 31.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 63.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 127.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.5')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.9')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.95')) to the cache location\n",
      "saving data for (('L', 1023.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 511.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.3')) to the cache location\n",
      "saving data for (('L', 255.0), ('D', 2.0), ('BCs', 0.0), ('H', '0.1')) to the cache location\n",
      "data saved to cache, see C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_cache/V1-8\n",
      "combing data in all folders\n",
      "merging C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_cache/V1-8\\H01D2B0\n",
      "merging C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_cache/V1-8\\H03D2B0\n",
      "merging C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_cache/V1-8\\H05D2B0\n",
      "merging C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_cache/V1-8\\H095D2B0\n",
      "merging C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_cache/V1-8\\H09D2B0\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process(version_param=\"V1-8\", source_format=source_format,target_format=target_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for g in glob(target_format.format(version_param)+\"//*\"):print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = list(data.keys())[0]\n",
    "l = [t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sets = {}\n",
    "for d in l:\n",
    "    k = make_key(d)\n",
    "    if k not in sets: sets[k] = []\n",
    "    sets[k].append(d)\n",
    "\n",
    "for k,tups in sets.items():\n",
    "    group = []\n",
    "    for t in tups:\n",
    "        d = data[t]\n",
    "        #qualify dataframe\n",
    "        L = \"{:0>5}\".format(int(dict(t)[\"L\"]))\n",
    "        \n",
    "        sorted_cols = d.columns.values\n",
    "        sorted_cols.sort()\n",
    "        \n",
    "        d = d[sorted_cols]\n",
    "        d.columns = [L+c for c in d.columns]\n",
    "        group.append(d)\n",
    "    group = pd.concat(group,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_lin = 'C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/BWS_VS_PROJECT/BWS/out.data'\n",
    "source_format = \"/home/clustor/ma/g/gunsim/bws_output/bws_{}*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"c:/Users/sirsh/Documents/DATA/L__H10D2B0.csv\", sep='\\t').drop(\"t\",1)#.drop(\"Unnamed: 0\",1)\n",
    "# df = df.tail(1).T\n",
    "# df.columns = [\"inf\"]\n",
    "# df[\"M\"] = \"sigma\"\n",
    "# df.loc[df.index.str.endswith(\"mu\"), \"M\"] = \"mu\"\n",
    "# df[\"sys\"] = df.index.str.split(\"_\").str.get(0)\n",
    "# df[\"Moment\"] = df.index.str.split(\"_\").str.get(1)\n",
    "# df.pivot(\"sys\",\"Moment\", \"inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check finall\n",
    "\n",
    "#pd.read_csv(\"./cache/L__H10D2B0.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def comment_line(file_name, line_num):\n",
    "#     lines = open(file_name, 'r').readlines()\n",
    "#     text = lines[line_num]\n",
    "#     text = \"#\" + text\n",
    "#     lines[line_num] = text\n",
    "#     out = open(file_name, 'w')\n",
    "#     out.writelines(lines)\n",
    "#     out.close()\n",
    "    \n",
    "# for f in glob(\"C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_output/*.*\"):\n",
    "#     comment_line(f, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_info(test_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#todo - standard error\n",
    "\n",
    "\n",
    "process(\"v1-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#grp = df.groupby([\"B\",\"D\", \"H\"])\n",
    "#for i,g in grp:\n",
    "#    print(i,g.file.values)\n",
    "#combiner(\"v1-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "source_format = \"/home/clustor/ma/g/gunsim/bws_output/bws_{}*\"\n",
    "source_format = \"C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_output/bws_{}*\"\n",
    "\n",
    "settings ={\n",
    "    \"cache_location\":\"./cache\", \n",
    "    \"param_space_format\" : \"L{0:i5}H{1:f}D{2:i}B{3:i}\",\n",
    "    \"grouping_keys\" : [\"L\", \"H\", \"D\",\"B\"],\n",
    "    \"cols_to_drop\" : [\"M0\", \"chunk\"],\n",
    "    \"combiner\" :{\n",
    "        \"reduce_on\" : [\"L\"],\n",
    "        \"filters\" : { \"L\" : [15,31] , \"D\" : [2,3,4,5] },\n",
    "        \"enabled\" : True\n",
    "    },\n",
    "    \"header_coerce_list\" : [\"Hopping rate\", \"Realisations\", \"Chunk size\"]  ,    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#todo - we should validate the inout files against the expectations above early on and give useful warnings!\n",
    "test_lin = 'C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/BWS_VS_PROJECT/BWS/out.data'\n",
    "#source_format = \"C:/Users/sirsh/Documents/Code/C/BWS_GITLAB/BranchingWienerProcess/data/bws_output/bws_{}*\"\n",
    "H_NAME= \"Hoppingrate\"\n",
    "\n",
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from os.path import basename\n",
    "import ast\n",
    "from itertools import product\n",
    "import re\n",
    "from string import Formatter\n",
    "  \n",
    "class MyFormatter(Formatter):\n",
    "    def format_field(self, value, format_spec):\n",
    "        if value == None:return \"__\"\n",
    "        #print(format_spec)\n",
    "        if format_spec == 'i':  # Truncate and render as int\n",
    "            return str(int(value))\n",
    "        if  format_spec[0] == 'i' and format_spec[1:].isdigit():\n",
    "            fstring = \"{:0\"+format_spec[1:]+\"d}\"\n",
    "            #print(fstring)\n",
    "            return fstring.format(int(value))\n",
    "        if format_spec == 'f':  # Truncate and render as int\n",
    "            value = int(str(round(float(value),2)).replace(\"0.\",\"\").replace(\".0\",\"\"))\n",
    "            fstring = \"{:<02}\"\n",
    "            return fstring.format(int(value))\n",
    "        #default\n",
    "        return super(MyFormatter, self).format_field(value, format_spec)\n",
    "\n",
    "    \n",
    "def process(key, skip_to_combine=False):\n",
    "    if skip_to_combine == False:\n",
    "        print(\"generating atoms for\",key)\n",
    "        l = list(get_file_list(key))\n",
    "\n",
    "        if len(l) ==0:\n",
    "            print(\"there are no files to process when matching\", key)\n",
    "            return\n",
    "\n",
    "        for f in l: reduce_file(f)\n",
    "   \n",
    "    print(\"\")\n",
    "    print(\"combining \", key)\n",
    "    \n",
    "    combiner(key)\n",
    "    \n",
    "    print(\"done!\")\n",
    "    \n",
    "def combiner(version_key):\n",
    "    #using settings, find a regex for file name and its parameters\n",
    "    #determine various filters \n",
    "    #find out what things we want to keep in file e.g. L\n",
    "    #for each such file load it and concatenate it horz\n",
    "    #save file according to format using a format that \n",
    "    \n",
    "    _dir = os.path.join(settings[\"cache_location\"], \"atoms\")\n",
    "    _dir = os.path.join(_dir, version_key)\n",
    "    glob_dir = os.path.join(_dir, \"*.*\")\n",
    "    print(\"combining files in \",glob_dir)\n",
    "    \n",
    "    file_from_key_desc = lambda f: os.path.join(_dir,f)\n",
    "    combiner_settings = settings[\"combiner\"]\n",
    "    reduction = []\n",
    "    if \"reduce_on\" in combiner_settings: reduction = combiner_settings[\"reduce_on\"]   \n",
    "    ordered_keys = settings[\"grouping_keys\"]\n",
    "    template = {}\n",
    "    for k in ordered_keys: template[k] = None\n",
    "    grouping_keys = [g for g in ordered_keys if g not in reduction]\n",
    "    \n",
    "    print(\"splittings groups in\", grouping_keys)\n",
    "    #get groups which are made of matching things that are not in the reduction\n",
    "    \n",
    "    #key the files, group by grouping keys\n",
    "    #iterate groups and get lists - concat them\n",
    "    #save as a file with a wildcard\n",
    "    \n",
    "    files = [f for f in glob(glob_dir)]\n",
    "    \n",
    "    print(\"processing \", len(files), \"files\")\n",
    "    keys = []\n",
    "    for f in files:\n",
    "        #use just the file name without path or extension\n",
    "        d = parameters_from_key(basename(f).split(\".\")[0])\n",
    "        d[\"file\"] =f\n",
    "        keys.append(d)\n",
    "    \n",
    "    #create a map of the file parameters\n",
    "    df = pd.DataFrame(keys)\n",
    "\n",
    "    print(\"grouping on keys\",grouping_keys)\n",
    "    #split them into groups\n",
    "    grp = df.groupby(grouping_keys)\n",
    "    for i,g in grp:\n",
    "        #create a template for the entire parameter space\n",
    "        _t = template.copy()\n",
    "        #for a template of parameter space, take the ordered value from the grouping\n",
    "        for ordinal, k in enumerate(grouping_keys): _t[k] = i[ordinal]\n",
    "            \n",
    "        print(\"combining on group\", _t)\n",
    "        #these null fields are the ones that are not in the group e.g. {L:None, H=0.95, B=0...}\n",
    "        null_keys = []\n",
    "        for k in _t.keys():\n",
    "            if _t[k] == None: null_keys.append(k)\n",
    "                \n",
    "        files = g.file.values\n",
    "        #print(files)\n",
    "        dfs = []\n",
    "        for _f in files:\n",
    "    \n",
    "            dkk = parameters_from_key(basename(_f).split(\".\")[0])\n",
    "            print(f,dkk)\n",
    "            #for tihs file name, figure out what is not in the template key\n",
    "            print(\"null keys are \", null_keys, \"and file keys are\", dkk)\n",
    "            \n",
    "            qual = \"\" #make a key for columns with the qualifier\n",
    "            for k in null_keys: \n",
    "                hack = k+dkk[k]\n",
    "                #this is a terrible hack just for L formatting - need to rethink all formatting\n",
    "                #if hack.startswith(\"L\"):  hack = str(str(hack.replace(\"L\", \"\")))\n",
    "                qual+= hack\n",
    "                \n",
    "            _df = pd.read_csv(_f).set_index(\"t\")\n",
    "            _df.columns = [qual+\"_\"+c for i, c in enumerate(_df.columns)]\n",
    "            dfs.append(_df)\n",
    "        #qualify the columns based on what group they came from\n",
    "        result = pd.concat(dfs,axis=1)\n",
    "        #result = result.set_index(result.t)#.drop(\"t\", 1)\n",
    "        print(\"sorting columns on moment ordering\")\n",
    "        cols = result.columns.values\n",
    "        cols = sorted(cols, key = lambda c : str(c))\n",
    "        result = result[cols]   \n",
    "        #append an integer col number AFTER choosing an ordering - RENAME\n",
    "        print(result.head())\n",
    "        result.columns =  [str(i) +\"_\"+c for i,c in enumerate(result.columns)]\n",
    "        \n",
    "        print(result.head())\n",
    "        \n",
    "        file_key = [_t[k] for k in ordered_keys]\n",
    "        file_name = MyFormatter().format(settings[\"param_space_format\"], *file_key)\n",
    "        \n",
    "        print(\"generating tail for large t\")\n",
    "        \n",
    "        #this is atemp hack : think about how this large-t generalises. I guess one want have to choose the primary axis only\n",
    "        df = result.tail(1).T\n",
    "        df.columns = [\"inf\"]\n",
    "        df[\"M\"] = \"sigma\"\n",
    "        df.loc[df.index.to_series().str.endswith(\"mu\"), \"M\"] = \"mu\"\n",
    "        #SYS is an optional BUt it might be necessary to do more complex disentanglement from mu/sigs\n",
    "        #if more dims\n",
    "        #THESE MAGIC NUMBERS 1 and 2 are the key parts for sys and moment. This shit is SOOOO danderous. \n",
    "        #Need to formalise the KEY system and encapsulate in a class ASAP!!\n",
    "        df[\"sys\"] = df.index.to_series().str.split(\"_\").str.get(1)\n",
    "        df[\"Moment\"] = df.index.to_series().str.split(\"_\").str.get(2)\n",
    "        df = df.pivot(\"sys\",\"Moment\", \"inf\")\n",
    "        \n",
    "\n",
    "        print(\"saving files to ...\", os.path.join(settings[\"cache_location\"]))\n",
    "        df.to_csv(os.path.join(os.path.join(settings[\"cache_location\"]), file_name + \"_tail.csv\"), sep='\\t')\n",
    "        #reset index to put t inplace and then used the regular index as row counter called 'index'\n",
    "        result = result.reset_index()\n",
    "        # SAVE WITHOUT IDEX COLUMN - T has been pop'd\n",
    "        result.to_csv(os.path.join(os.path.join(settings[\"cache_location\"]), file_name + \".csv\"), sep='\\t', index=False)\n",
    "\n",
    "    return pd.DataFrame(keys)\n",
    "    \n",
    "    \n",
    "\n",
    "def parameters_from_key(s):\n",
    "    \"\"\"\n",
    "    the key MUST be a combination of words and numbers: AA12B12C987D93\n",
    "    the config knows which things are floats based on the format string\n",
    "    and this can be used to recover the floating point number\n",
    "    \"\"\"\n",
    "    return dict(re.findall(r\"([a-z]+)([0-9]+)\", s, re.I))\n",
    "\n",
    "\n",
    "def get_file_list(vkey, format_string =source_format):\n",
    "    search_path = format_string.format(vkey)\n",
    "    print(\"searching for files\", search_path)\n",
    "    for f in glob(search_path):\n",
    "        yield f\n",
    "        \n",
    "def _parse_params(l):\n",
    "    \"\"\"\n",
    "    Assumes parameter line is a valid dictionary/json like format and parses it \n",
    "    otherwise just returns the full line\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if l.replace(\" \", \"\").startswith(\"#Parameters\"):\n",
    "        l = l.replace(\"#Parameters=\", \"\")\n",
    "        l = l.replace(\"# Parameters=\", \"\")\n",
    "        l=l.replace(\" \",\"\").replace(\"(\",\"\").replace(\")\",\"\")            \n",
    "        try:\n",
    "            d = ast.literal_eval(l)\n",
    "            #if we needed to map names we would do it here\n",
    "            return d\n",
    "        \n",
    "        except:\n",
    "            try:\n",
    "                #try look for terms\n",
    "                terms = l.replace(\"{\",\"\").replace(\"}\",\"\").split(\",\")\n",
    "                for t in terms:\n",
    "                    a,b = t.split(\":\")\n",
    "                    a = a.lstrip().rstrip().replace(\" \",\"\")\n",
    "                    #print(\"parsing terms\",a)\n",
    "                    #we use the non-spaced tokens\n",
    "                    cl_no_space = [s.replace(\" \",\"\") for s in settings[\"header_coerce_list\"]]\n",
    "                    if a in cl_no_space: \n",
    "                        d[a] = b\n",
    "            except:\n",
    "                return {\"unparsed\", p}\n",
    "            \n",
    "    return d\n",
    "        \n",
    "def read_info(file, param_line=\"#Parameters\",sep='\\t'):\n",
    "    with open(file) as _f:\n",
    "        d = {\"version\": \"default\"}\n",
    "        #TODO: extract version and other info from file header\n",
    "        \n",
    "        tokens = file.split(\"-\") #should use a smarter regex maybe    \n",
    "        if len(tokens) > 1: d[\"version\"]= tokens[0].split(\"_\")[-1]+\"-\"+tokens[1]\n",
    "        for i,line in enumerate(_f):\n",
    "            if line.startswith(param_line):\n",
    "                d.update(_parse_params(line))                    \n",
    "            if i == 10:#some reasonable comment scanning\n",
    "                break\n",
    "        d[\"file\"] = file\n",
    "        return d  #{\"file\":f, \"L\" : None, \"D\": None, \"BCs\" : None, \"seed\" : None, \"N\":None, \"h\":None, \"T\"} #info\n",
    "    \n",
    "def _format_keys(keys):\n",
    "    ordered_keys = settings[\"grouping_keys\"]\n",
    "    vals = [keys[k] for k in ordered_keys if k in keys]\n",
    "    print(\"formating keys \", settings[\"param_space_format\"],vals)\n",
    "    return MyFormatter().format(settings[\"param_space_format\"], *vals)\n",
    "    \n",
    "def iter_coordinate_frame(df):\n",
    "    print(\"checking parameter space of dataframe for space \", str(settings[\"grouping_keys\"]))\n",
    "    space = {}\n",
    "    \n",
    "    for c in settings[\"grouping_keys\"]:\n",
    "        if c in df.columns:\n",
    "            space [c] = list(df[c].unique())\n",
    "            \n",
    "    print(\"found space \",space, \"\\nproducing cartesian product parameter space data...\")\n",
    "    \n",
    "    keys = list(space.keys())\n",
    "    lists = [space[k] for k in keys]\n",
    "    P = list(product(*lists))\n",
    "    print(\"We have coordinates (\"+str(keys)+\"): \"+str(P))   \n",
    "    print(\"slicing dataframes for each of these coords...\")\n",
    "    for i,c in enumerate(P):\n",
    "        map_keys = dict(zip(keys,list(c)))\n",
    "        print( str(i)+\":\", str(map_keys))   \n",
    "        print(\"generated a key: \",_format_keys(map_keys))\n",
    "        yield _format_keys(map_keys), df\n",
    "    \n",
    "def reduce_file(f):\n",
    "    \"\"\"\n",
    "    reduce accross chunks on other categorical columns. capture statistics for Moments M0+\n",
    "    because we might have multiple keys per dataframe, for simplicity with split them here and reduce to one coord\n",
    "    \"\"\"\n",
    "    _df = None\n",
    "    \n",
    "    metadata = read_info(f)  \n",
    "    print(\"reducing file\",f)\n",
    "    print(\"HEADER:\")\n",
    "    print(metadata)\n",
    "    print(\"*************************\")\n",
    "    #df = pd.read_csv(f, comment=\"#\",sep='\\t',dtype=float)   \n",
    "    df = read_df_custom(f)\n",
    "    #add or modifiy columns on the data datafram\n",
    "    #TODO: broken abstraction: we want to use different names for our params - in generall 1/2 chars upper case preferred\n",
    "    df[\"H\"] = metadata[H_NAME]\n",
    "    #rename from what we called them in the output\n",
    "    df = df.rename(columns={\"BCs\":\"B\"})\n",
    "    \n",
    "    for key, _df in iter_coordinate_frame(df):\n",
    "        _df = reduce_df(_df)\n",
    "        _dir = os.path.join(settings[\"cache_location\"],\"atoms\")\n",
    "        if metadata[\"version\"] != None: _dir = os.path.join(_dir, metadata[\"version\"])\n",
    "        _try_make_dir(_dir)\n",
    "        _file = os.path.join(_dir, key+\".csv\")\n",
    "        print(\"saving file\", _file)\n",
    "        #method to write comments in header\n",
    "        #http://stackoverflow.com/questions/29233496/write-comments-in-csv-file-with-pandas\n",
    "        _df.to_csv(_file)\n",
    "    #returns both the dataframe and the version for accumulation\n",
    "    return _df, metadata[\"version\"]\n",
    "\n",
    "def read_df_custom(f):\n",
    "    lines = []\n",
    "    #print(\"opening file \", f)\n",
    "    with open(f) as fl:\n",
    "        for i,l in enumerate(fl):\n",
    "            if l.startswith('#'):\n",
    "                continue\n",
    "            l = l.replace(\"\\n\", \"\").replace('\\n', \"\")\n",
    "            lines.append(l.split('\\t'))\n",
    "    df = pd.DataFrame([l for l in lines[1:-1]], columns = lines[0] ).astype(float)\n",
    "    return df\n",
    "    \n",
    "def reduce_df(df):   \n",
    "    moments = [\"M\"+str(i) for i in range(1,9)]\n",
    "    print(\"normalising moments \",str(moments))\n",
    "    for m in moments: df[m] /= df[\"M0\"]\n",
    "    print(\"checking grouping keys {} are in dataframe\".format(str(settings[\"grouping_keys\"])))\n",
    "    gkeys = [g for g in settings[\"grouping_keys\"] if g in df.columns]\n",
    "    #add t to the list of things to group by \n",
    "    grp = df.groupby(gkeys + [\"t\"])\n",
    "    df_mean = grp.mean().reset_index().set_index(\"t\")\n",
    "    #sample a chunk value\n",
    "    N = grp.count()[\"M0\"].values[0]\n",
    "    #standard error\n",
    "    df_std = grp.std() / np.sqrt(N)\n",
    "    #setup the join index\n",
    "    df_std =df_std.reset_index().set_index(\"t\")\n",
    "    print(\"dropping columns; moments \",str(settings[\"cols_to_drop\"]))\n",
    "    for c in settings[\"cols_to_drop\"]:\n",
    "        if c in df_mean.columns: df_mean = df_mean.drop(c,1)\n",
    "        if c in df_std.columns: df_std = df_std.drop(c,1)\n",
    "            \n",
    "    print(\"dropping columns in the grouping, post grouping \",gkeys)\n",
    "    for c in gkeys:\n",
    "        if c in df_mean.columns: df_mean = df_mean.drop(c,1)\n",
    "        if c in df_std.columns: df_std = df_std.drop(c,1)\n",
    "\n",
    "    result = df_mean.join(df_std, lsuffix=\"mu\", rsuffix=\"sigma\")\n",
    "    \n",
    "    print(\"sorting columns on moment ordering\")\n",
    "    cols = result.columns.values\n",
    "    cols.sort()\n",
    "\n",
    "    result = result[cols]\n",
    "    return result\n",
    "\n",
    "def collect_hist(key, cache=False):\n",
    "    #go through the files and save /histograms or return them for plotting with their key\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_helper(key, data=None):\n",
    "    \"\"\"\n",
    "    if data is none, try to load from cache\n",
    "    plotting type can be added to key/dictionary\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def _try_make_dir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(test_lin,sep='\\t', comment='#')\n",
    "#res,_ = reduce_file(test_lin)\n",
    "process(\"v1-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
